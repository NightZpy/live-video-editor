# Configuraci√≥n de Whisper: Est√°ndar vs Faster-Whisper

Este proyecto soporta dos implementaciones de Whisper para transcripci√≥n de audio:

## üîß Implementaciones Disponibles

### 1. **Whisper Est√°ndar** (Original OpenAI)
- ‚úÖ Implementaci√≥n oficial de OpenAI
- ‚úÖ M√°xima compatibilidad
- ‚ö†Ô∏è Velocidad est√°ndar
- ‚ö†Ô∏è Mayor uso de memoria

### 2. **Faster-Whisper** (Optimizado)
- ‚úÖ **4-8x m√°s r√°pido** que Whisper est√°ndar
- ‚úÖ **Menor uso de memoria** (~50% menos)
- ‚úÖ **Misma precisi√≥n** que Whisper est√°ndar
- ‚úÖ Soporte para Voice Activity Detection (VAD)
- ‚úÖ Optimizado para CPU y GPU

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno (.env)

```bash
# Implementaci√≥n a usar
USE_FASTER_WHISPER=true    # true = Faster-Whisper, false = Whisper est√°ndar

# Modelo Whisper a usar (recomendado: large-v3)
WHISPER_MODEL=large-v3     # large-v3, large, medium, small, base
```

### Modelos Disponibles (ambas implementaciones)

| Modelo | Tama√±o | Velocidad | Precisi√≥n | Recomendado para |
|--------|--------|-----------|-----------|------------------|
| `large-v3` | ~3GB | Lento | **M√°xima** | **Videos importantes** |
| `large` | ~3GB | Lento | Muy Alta | Videos largos de calidad |
| `medium` | ~1.5GB | Medio | Alta | Balance velocidad/calidad |
| `small` | ~500MB | R√°pido | Media | Pruebas r√°pidas |
| `base` | ~150MB | Muy R√°pido | B√°sica | Solo para testing |

## üöÄ Instalaci√≥n

### Opci√≥n 1: Script Autom√°tico (Recomendado)

```bash
# Linux/macOS/WSL
./install-whisper.sh

# Windows
install-whisper.bat
```

### Opci√≥n 2: Manual

```bash
# Instalar dependencias base
pip install -r requirements-base.txt

# Instalar Faster-Whisper
pip install faster-whisper>=1.1.0
```

## üìä Comparaci√≥n de Rendimiento

| M√©trica | Whisper Est√°ndar | Faster-Whisper | Mejora |
|---------|------------------|----------------|---------|
| **Velocidad** | 1x | **4-8x** | 400-800% |
| **Memoria** | 100% | **~50%** | 50% menos |
| **Precisi√≥n** | 100% | **100%** | Igual |
| **Calidad** | Excelente | **Excelente** | Igual |

## üéØ Configuraciones Recomendadas

### Para M√°xima Calidad
```bash
USE_FASTER_WHISPER=true
WHISPER_MODEL=large-v3
```

### Para Balance Velocidad/Calidad
```bash
USE_FASTER_WHISPER=true
WHISPER_MODEL=medium
```

### Para M√°xima Compatibilidad
```bash
USE_FASTER_WHISPER=false
WHISPER_MODEL=large-v3
```

## üîç Verificaci√≥n de Configuraci√≥n

El sistema autom√°ticamente detecta y reporta la configuraci√≥n al iniciar:

```
üîß Whisper transcriber initialized:
   - Implementation: Faster-Whisper
   - Preferred model: large-v3
   - Device: cuda
```

## üö® Resoluci√≥n de Problemas

### Faster-Whisper no disponible
Si `USE_FASTER_WHISPER=true` pero faster-whisper no est√° instalado:
- El sistema autom√°ticamente fallback a Whisper est√°ndar
- Instalar: `pip install faster-whisper>=1.1.0`

### Modelo no encontrado
Si el modelo especificado no est√° disponible:
- El sistema prueba modelos alternativos autom√°ticamente
- Orden de fallback: `large-v3` ‚Üí `large` ‚Üí `medium` ‚Üí `small` ‚Üí `base`

### Memoria insuficiente
Para problemas de memoria GPU/CPU:
- Usar modelo m√°s peque√±o: `WHISPER_MODEL=medium`
- Usar CPU en lugar de GPU
- Usar Faster-Whisper para menor uso de memoria

## üí° Recomendaciones

1. **Para producci√≥n**: `USE_FASTER_WHISPER=true` + `WHISPER_MODEL=large-v3`
2. **Para desarrollo**: `USE_FASTER_WHISPER=true` + `WHISPER_MODEL=medium`  
3. **Para testing**: `USE_FASTER_WHISPER=true` + `WHISPER_MODEL=small`

El resto del pipeline (PyTorch, callbacks, cach√©, an√°lisis LLM) funciona **exactamente igual** con ambas implementaciones.
